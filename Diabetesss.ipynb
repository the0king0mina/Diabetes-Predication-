{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قبل المعالجة:\n",
      " 0    91482\n",
      "1     8500\n",
      "Name: diabetes, dtype: int64\n",
      "بعد المعالجة:\n",
      " 1    8500\n",
      "0    8500\n",
      "Name: diabetes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"C:/Users/MINA/Downloads/diabetes.csv\")\n",
    "df\n",
    "\n",
    "# حساب توزيع الفئات\n",
    "class_counts = df[\"diabetes\"].value_counts()\n",
    "print(\"قبل المعالجة:/n\", class_counts)\n",
    "\n",
    "# فصل الفئات\n",
    "df_majority = df[df[\"diabetes\"] == 0]  # الفئة الكبرى (91482)\n",
    "df_minority = df[df[\"diabetes\"] == 1]  # الفئة الصغرى (8500)\n",
    "\n",
    "# اختيار عينة عشوائية من الفئة الكبرى بحجم الفئة الصغرى (8500)\n",
    "df_majority_downsampled = df_majority.sample(n=len(df_minority), random_state=42)\n",
    "\n",
    "# دمج البيانات مرة أخرى\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# إعادة ترتيب البيانات عشوائيًا\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# التحقق من التوزيع بعد المعالجة\n",
    "print(\"بعد المعالجة:/n\", df_balanced[\"diabetes\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"C:/Users/MINA/Downloads/diabetes.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17000 entries, 0 to 16999\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   gender               17000 non-null  object \n",
      " 1   age                  17000 non-null  float64\n",
      " 2   hypertension         17000 non-null  int64  \n",
      " 3   heart_disease        17000 non-null  int64  \n",
      " 4   smoking_history      17000 non-null  object \n",
      " 5   bmi                  17000 non-null  float64\n",
      " 6   HbA1c_level          17000 non-null  float64\n",
      " 7   blood_glucose_level  17000 non-null  int64  \n",
      " 8   diabetes             17000 non-null  int64  \n",
      "dtypes: float64(3), int64(4), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_balanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature=[\"HbA1c_level\",\"blood_glucose_level\",\"bmi\",\"age\",\"hypertension\",\"heart_disease\"]\n",
    "X=df_balanced[selected_feature]\n",
    "y=df_balanced[\"diabetes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler=StandardScaler()\n",
    "#X=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "أفضل المعلمات: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1}\n",
      "\n",
      "Accuracy 0.9117994100294985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200,300,500],  # عدد الأشجار\n",
    "    'learning_rate': [0.01, 0.1, 0.2,0.9,0.6],  # معدل التعلم\n",
    "    'max_depth': [3, 5, 7,9,12],  # عمق الشجرة\n",
    "    'subsample': [0.8, 1],  # نسبة استخدام البيانات لكل شجرة\n",
    "    'colsample_bytree': [0.8, 1]  # نسبة استخدام الميزات لكل شجرة\n",
    "}\n",
    "\n",
    "# إنشاء نموذج XGBClassifier\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# تطبيق GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',  # يمكنك تغييرها إلى f1 أو recall حسب الحاجة\n",
    "    cv=5,  # عدد الـ folds\n",
    "    n_jobs=-1,  # استخدام جميع المعالجات لتسريع العملية\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# **تمرير الأوزان يدويًا أثناء fit**\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# أفضل المعلمات\n",
    "print(\"أفضل المعلمات:\", grid_search.best_params_)\n",
    "\n",
    "# أفضل نموذج بعد البحث\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# التقييم\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nAccuracy\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mbest_model\u001b[49m\n\u001b[0;32m      2\u001b[0m y_predict\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      3\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "model=best_model\n",
    "y_predict=model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"دقة النموذج:\", accuracy)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rf_model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دقة النموذج: 0.8982300884955752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      1693\n",
      "           1       0.90      0.90      0.90      1697\n",
      "\n",
      "    accuracy                           0.90      3390\n",
      "   macro avg       0.90      0.90      0.90      3390\n",
      "weighted avg       0.90      0.90      0.90      3390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_prrrrrr=rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_prrrrrr)\n",
    "print(\"دقة النموذج:\", accuracy)\n",
    "\n",
    "print(classification_report(y_test, y_prrrrrr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,049,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,025\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,016,065</span> (7.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,016,065\u001b[0m (7.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,014,017</span> (7.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,014,017\u001b[0m (7.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "\n",
    "\n",
    "     Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "     Dense(128, activation=\"relu\"),\n",
    "     Dense(256, activation=\"relu\"),\n",
    "     Dense(512, activation=\"relu\"),\n",
    "     Dense(512, activation=\"relu\"),\n",
    "     Dropout(0.2),\n",
    "     Dense(1024, activation=\"relu\"),\n",
    "     Dense(1024, activation=\"relu\"),\n",
    "     Dropout(0.2),\n",
    "     BatchNormalization(),\n",
    "\n",
    "     Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adamax(0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 16ms/step - accuracy: 0.8401 - loss: 0.3602 - val_accuracy: 0.8953 - val_loss: 0.2140\n",
      "Epoch 2/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 17ms/step - accuracy: 0.8634 - loss: 0.3008 - val_accuracy: 0.8958 - val_loss: 0.2319\n",
      "Epoch 3/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.8813 - loss: 0.2611 - val_accuracy: 0.9031 - val_loss: 0.1947\n",
      "Epoch 4/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8837 - loss: 0.2644 - val_accuracy: 0.9046 - val_loss: 0.1939\n",
      "Epoch 5/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8901 - loss: 0.2512 - val_accuracy: 0.9027 - val_loss: 0.1947\n",
      "Epoch 6/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.8903 - loss: 0.2553 - val_accuracy: 0.9095 - val_loss: 0.2000\n",
      "Epoch 7/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.8889 - loss: 0.2457 - val_accuracy: 0.9051 - val_loss: 0.1949\n",
      "Epoch 8/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.8902 - loss: 0.2405 - val_accuracy: 0.8997 - val_loss: 0.1993\n",
      "Epoch 9/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.8877 - loss: 0.2481 - val_accuracy: 0.9046 - val_loss: 0.1985\n",
      "Epoch 10/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.8966 - loss: 0.2348 - val_accuracy: 0.9007 - val_loss: 0.2022\n",
      "Epoch 11/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.8890 - loss: 0.2432 - val_accuracy: 0.9022 - val_loss: 0.1998\n",
      "Epoch 12/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 17ms/step - accuracy: 0.8919 - loss: 0.2357 - val_accuracy: 0.9100 - val_loss: 0.1931\n",
      "Epoch 13/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 17ms/step - accuracy: 0.8980 - loss: 0.2278 - val_accuracy: 0.9081 - val_loss: 0.2029\n",
      "Epoch 14/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step - accuracy: 0.8998 - loss: 0.2263 - val_accuracy: 0.8992 - val_loss: 0.1991\n",
      "Epoch 15/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 17ms/step - accuracy: 0.8952 - loss: 0.2364 - val_accuracy: 0.9090 - val_loss: 0.1921\n",
      "Epoch 16/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.8949 - loss: 0.2299 - val_accuracy: 0.9027 - val_loss: 0.1951\n",
      "Epoch 17/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8934 - loss: 0.2345 - val_accuracy: 0.9051 - val_loss: 0.1987\n",
      "Epoch 18/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8921 - loss: 0.2374 - val_accuracy: 0.8830 - val_loss: 0.2476\n",
      "Epoch 19/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8914 - loss: 0.2439 - val_accuracy: 0.9031 - val_loss: 0.1932\n",
      "Epoch 20/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8906 - loss: 0.2325 - val_accuracy: 0.9081 - val_loss: 0.1888\n",
      "Epoch 21/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8984 - loss: 0.2310 - val_accuracy: 0.9066 - val_loss: 0.1955\n",
      "Epoch 22/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8957 - loss: 0.2299 - val_accuracy: 0.8899 - val_loss: 0.2052\n",
      "Epoch 23/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8955 - loss: 0.2272 - val_accuracy: 0.8894 - val_loss: 0.2089\n",
      "Epoch 24/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8943 - loss: 0.2270 - val_accuracy: 0.9046 - val_loss: 0.1974\n",
      "Epoch 25/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8992 - loss: 0.2191 - val_accuracy: 0.8987 - val_loss: 0.1910\n",
      "Epoch 26/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8940 - loss: 0.2275 - val_accuracy: 0.9066 - val_loss: 0.1939\n",
      "Epoch 27/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8911 - loss: 0.2303 - val_accuracy: 0.9041 - val_loss: 0.2059\n",
      "Epoch 28/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8925 - loss: 0.2274 - val_accuracy: 0.8987 - val_loss: 0.2039\n",
      "Epoch 29/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8900 - loss: 0.2332 - val_accuracy: 0.9071 - val_loss: 0.1991\n",
      "Epoch 30/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.9009 - loss: 0.2266 - val_accuracy: 0.9051 - val_loss: 0.1994\n",
      "Epoch 31/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.9007 - loss: 0.2130 - val_accuracy: 0.8977 - val_loss: 0.1984\n",
      "Epoch 32/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8965 - loss: 0.2245 - val_accuracy: 0.8977 - val_loss: 0.2021\n",
      "Epoch 33/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8943 - loss: 0.2287 - val_accuracy: 0.8953 - val_loss: 0.1974\n",
      "Epoch 34/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8906 - loss: 0.2319 - val_accuracy: 0.9031 - val_loss: 0.1947\n",
      "Epoch 35/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8977 - loss: 0.2187 - val_accuracy: 0.9012 - val_loss: 0.2119\n",
      "Epoch 36/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.9000 - loss: 0.2217 - val_accuracy: 0.8992 - val_loss: 0.2234\n",
      "Epoch 37/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8915 - loss: 0.2327 - val_accuracy: 0.8943 - val_loss: 0.1997\n",
      "Epoch 38/100\n",
      "\u001b[1m1441/1441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.8989 - loss: 0.2214 - val_accuracy: 0.8958 - val_loss: 0.2064\n",
      "Epoch 39/100\n",
      "\u001b[1m 929/1441\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.8950 - loss: 0.2283"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.15, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optree\\ops.py:747\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    745\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    746\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 03-15 16:37:58] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 03-15 16:37:58] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 03-15 16:37:58] {1838} INFO - Minimizing error metric: 1-roc_auc\n",
      "[flaml.automl.logger: 03-15 16:37:58] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 03-15 16:37:58] {2258} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2393} INFO - Estimated sufficient time budget=1761s. Estimated necessary time budget=43s.\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2442} INFO -  at 0.3s,\testimator lgbm's best error=0.0528,\tbest estimator lgbm's best error=0.0528\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2442} INFO -  at 0.5s,\testimator lgbm's best error=0.0528,\tbest estimator lgbm's best error=0.0528\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2258} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2442} INFO -  at 0.7s,\testimator lgbm's best error=0.0370,\tbest estimator lgbm's best error=0.0370\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2258} INFO - iteration 3, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2442} INFO -  at 0.8s,\testimator sgd's best error=0.0506,\tbest estimator lgbm's best error=0.0370\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2442} INFO -  at 1.1s,\testimator lgbm's best error=0.0233,\tbest estimator lgbm's best error=0.0233\n",
      "[flaml.automl.logger: 03-15 16:37:59] {2258} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2442} INFO -  at 1.3s,\testimator lgbm's best error=0.0233,\tbest estimator lgbm's best error=0.0233\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2258} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2442} INFO -  at 1.6s,\testimator lgbm's best error=0.0214,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2442} INFO -  at 1.9s,\testimator lgbm's best error=0.0214,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2258} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2442} INFO -  at 2.1s,\testimator lgbm's best error=0.0214,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2258} INFO - iteration 9, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2442} INFO -  at 2.2s,\testimator sgd's best error=0.0492,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:00] {2258} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2442} INFO -  at 2.4s,\testimator xgboost's best error=0.0528,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2258} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2442} INFO -  at 2.6s,\testimator xgboost's best error=0.0492,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2258} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2442} INFO -  at 2.9s,\testimator extra_tree's best error=0.0974,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2258} INFO - iteration 13, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2442} INFO -  at 3.1s,\testimator rf's best error=0.0561,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:01] {2258} INFO - iteration 14, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:02] {2442} INFO -  at 3.4s,\testimator rf's best error=0.0380,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:02] {2258} INFO - iteration 15, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:02] {2442} INFO -  at 3.6s,\testimator rf's best error=0.0380,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:02] {2258} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:38:02] {2442} INFO -  at 3.8s,\testimator extra_tree's best error=0.0825,\tbest estimator lgbm's best error=0.0214\n",
      "[flaml.automl.logger: 03-15 16:38:02] {2258} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:03] {2442} INFO -  at 4.7s,\testimator lgbm's best error=0.0207,\tbest estimator lgbm's best error=0.0207\n",
      "[flaml.automl.logger: 03-15 16:38:03] {2258} INFO - iteration 18, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:38:03] {2442} INFO -  at 5.0s,\testimator extra_tree's best error=0.0825,\tbest estimator lgbm's best error=0.0207\n",
      "[flaml.automl.logger: 03-15 16:38:03] {2258} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:04] {2442} INFO -  at 5.3s,\testimator lgbm's best error=0.0207,\tbest estimator lgbm's best error=0.0207\n",
      "[flaml.automl.logger: 03-15 16:38:04] {2258} INFO - iteration 20, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:04] {2442} INFO -  at 5.6s,\testimator rf's best error=0.0380,\tbest estimator lgbm's best error=0.0207\n",
      "[flaml.automl.logger: 03-15 16:38:04] {2258} INFO - iteration 21, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:04] {2442} INFO -  at 5.9s,\testimator rf's best error=0.0303,\tbest estimator lgbm's best error=0.0207\n",
      "[flaml.automl.logger: 03-15 16:38:04] {2258} INFO - iteration 22, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:05] {2442} INFO -  at 6.3s,\testimator rf's best error=0.0289,\tbest estimator lgbm's best error=0.0207\n",
      "[flaml.automl.logger: 03-15 16:38:05] {2258} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:08] {2442} INFO -  at 9.2s,\testimator lgbm's best error=0.0207,\tbest estimator lgbm's best error=0.0207\n",
      "[flaml.automl.logger: 03-15 16:38:08] {2258} INFO - iteration 24, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:38:11] {2442} INFO -  at 13.1s,\testimator catboost's best error=0.0205,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:11] {2258} INFO - iteration 25, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:12] {2442} INFO -  at 13.4s,\testimator rf's best error=0.0289,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:12] {2258} INFO - iteration 26, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:38:13] {2442} INFO -  at 14.8s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:13] {2258} INFO - iteration 27, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:38:18] {2442} INFO -  at 19.7s,\testimator catboost's best error=0.0205,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:18] {2258} INFO - iteration 28, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:19] {2442} INFO -  at 20.3s,\testimator rf's best error=0.0252,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:19] {2258} INFO - iteration 29, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:19] {2442} INFO -  at 20.4s,\testimator xgboost's best error=0.0492,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:19] {2258} INFO - iteration 30, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:38:21] {2442} INFO -  at 22.8s,\testimator catboost's best error=0.0205,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:21] {2258} INFO - iteration 31, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:38:27] {2442} INFO -  at 28.2s,\testimator catboost's best error=0.0205,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:27] {2258} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:28] {2442} INFO -  at 29.7s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:28] {2258} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:29] {2442} INFO -  at 30.9s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:29] {2258} INFO - iteration 34, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:30] {2442} INFO -  at 31.2s,\testimator rf's best error=0.0252,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:30] {2258} INFO - iteration 35, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:38:30] {2442} INFO -  at 31.7s,\testimator extra_tree's best error=0.0820,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:30] {2258} INFO - iteration 36, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:32] {2442} INFO -  at 33.4s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:32] {2258} INFO - iteration 37, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:32] {2442} INFO -  at 33.9s,\testimator rf's best error=0.0252,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:32] {2258} INFO - iteration 38, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:38:37] {2442} INFO -  at 39.2s,\testimator catboost's best error=0.0205,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:37] {2258} INFO - iteration 39, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:38] {2442} INFO -  at 39.7s,\testimator rf's best error=0.0232,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:38] {2258} INFO - iteration 40, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:38] {2442} INFO -  at 39.9s,\testimator xgboost's best error=0.0348,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:38] {2258} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:38] {2442} INFO -  at 40.1s,\testimator xgboost's best error=0.0302,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:38] {2258} INFO - iteration 42, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:39] {2442} INFO -  at 40.4s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:39] {2258} INFO - iteration 43, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:39] {2442} INFO -  at 40.8s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:39] {2258} INFO - iteration 44, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:40] {2442} INFO -  at 41.7s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:40] {2258} INFO - iteration 45, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:40] {2442} INFO -  at 42.1s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0205\n",
      "[flaml.automl.logger: 03-15 16:38:40] {2258} INFO - iteration 46, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2442} INFO -  at 46.5s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2258} INFO - iteration 47, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2442} INFO -  at 46.7s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2258} INFO - iteration 48, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2442} INFO -  at 47.0s,\testimator rf's best error=0.0232,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2258} INFO - iteration 49, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2442} INFO -  at 47.2s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:45] {2258} INFO - iteration 50, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:46] {2442} INFO -  at 48.0s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:46] {2258} INFO - iteration 51, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:47] {2442} INFO -  at 48.7s,\testimator rf's best error=0.0222,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:47] {2258} INFO - iteration 52, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:38:52] {2442} INFO -  at 53.9s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:52] {2258} INFO - iteration 53, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:53] {2442} INFO -  at 54.5s,\testimator rf's best error=0.0222,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:53] {2258} INFO - iteration 54, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:54] {2442} INFO -  at 55.5s,\testimator rf's best error=0.0222,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:54] {2258} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:55] {2442} INFO -  at 56.6s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:55] {2258} INFO - iteration 56, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:56] {2442} INFO -  at 57.2s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:56] {2258} INFO - iteration 57, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:56] {2442} INFO -  at 57.4s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:56] {2258} INFO - iteration 58, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:38:57] {2442} INFO -  at 58.8s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:57] {2258} INFO - iteration 59, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:38:58] {2442} INFO -  at 59.4s,\testimator rf's best error=0.0222,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:58] {2258} INFO - iteration 60, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:38:58] {2442} INFO -  at 59.8s,\testimator xgb_limitdepth's best error=0.0240,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:58] {2258} INFO - iteration 61, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:38:58] {2442} INFO -  at 60.2s,\testimator xgb_limitdepth's best error=0.0240,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:58] {2258} INFO - iteration 62, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:38:59] {2442} INFO -  at 60.3s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:59] {2258} INFO - iteration 63, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:38:59] {2442} INFO -  at 60.8s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:59] {2258} INFO - iteration 64, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:38:59] {2442} INFO -  at 61.2s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:38:59] {2258} INFO - iteration 65, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:00] {2442} INFO -  at 61.6s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:00] {2258} INFO - iteration 66, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:00] {2442} INFO -  at 61.8s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:00] {2258} INFO - iteration 67, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:39:01] {2442} INFO -  at 62.5s,\testimator rf's best error=0.0222,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:01] {2258} INFO - iteration 68, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:02] {2442} INFO -  at 63.6s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:02] {2258} INFO - iteration 69, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:03] {2442} INFO -  at 64.3s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:03] {2258} INFO - iteration 70, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:03] {2442} INFO -  at 64.6s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:03] {2258} INFO - iteration 71, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:39:03] {2442} INFO -  at 65.0s,\testimator rf's best error=0.0222,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:03] {2258} INFO - iteration 72, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:04] {2442} INFO -  at 65.5s,\testimator xgb_limitdepth's best error=0.0218,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:04] {2258} INFO - iteration 73, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:04] {2442} INFO -  at 65.9s,\testimator xgb_limitdepth's best error=0.0217,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:04] {2258} INFO - iteration 74, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:39:04] {2442} INFO -  at 66.2s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:04] {2258} INFO - iteration 75, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:09] {2442} INFO -  at 70.7s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:09] {2258} INFO - iteration 76, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:09] {2442} INFO -  at 71.0s,\testimator extra_tree's best error=0.0611,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:09] {2258} INFO - iteration 77, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:10] {2442} INFO -  at 71.4s,\testimator extra_tree's best error=0.0611,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:10] {2258} INFO - iteration 78, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:39:10] {2442} INFO -  at 71.6s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:10] {2258} INFO - iteration 79, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:39:10] {2442} INFO -  at 72.0s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:10] {2258} INFO - iteration 80, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:11] {2442} INFO -  at 72.3s,\testimator extra_tree's best error=0.0602,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:11] {2258} INFO - iteration 81, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:39:11] {2442} INFO -  at 72.7s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:11] {2258} INFO - iteration 82, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:39:12] {2442} INFO -  at 73.5s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:12] {2258} INFO - iteration 83, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:16] {2442} INFO -  at 77.7s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:16] {2258} INFO - iteration 84, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:16] {2442} INFO -  at 78.1s,\testimator extra_tree's best error=0.0322,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:16] {2258} INFO - iteration 85, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:17] {2442} INFO -  at 78.4s,\testimator extra_tree's best error=0.0322,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:17] {2258} INFO - iteration 86, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:17] {2442} INFO -  at 78.7s,\testimator extra_tree's best error=0.0322,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:17] {2258} INFO - iteration 87, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:17] {2442} INFO -  at 79.2s,\testimator extra_tree's best error=0.0322,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:17] {2258} INFO - iteration 88, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:18] {2442} INFO -  at 79.5s,\testimator extra_tree's best error=0.0322,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:18] {2258} INFO - iteration 89, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:18] {2442} INFO -  at 79.9s,\testimator extra_tree's best error=0.0290,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:18] {2258} INFO - iteration 90, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:19] {2442} INFO -  at 80.4s,\testimator extra_tree's best error=0.0290,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:19] {2258} INFO - iteration 91, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:19] {2442} INFO -  at 80.9s,\testimator extra_tree's best error=0.0290,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:19] {2258} INFO - iteration 92, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:23] {2442} INFO -  at 84.9s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:23] {2258} INFO - iteration 93, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:24] {2442} INFO -  at 85.2s,\testimator extra_tree's best error=0.0269,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:24] {2258} INFO - iteration 94, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:27] {2442} INFO -  at 88.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:27] {2258} INFO - iteration 95, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:27] {2442} INFO -  at 89.2s,\testimator extra_tree's best error=0.0269,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:27] {2258} INFO - iteration 96, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:28] {2442} INFO -  at 89.7s,\testimator extra_tree's best error=0.0269,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:28] {2258} INFO - iteration 97, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:39:28] {2442} INFO -  at 90.0s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:28] {2258} INFO - iteration 98, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:34] {2442} INFO -  at 96.0s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:34] {2258} INFO - iteration 99, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:35] {2442} INFO -  at 96.5s,\testimator extra_tree's best error=0.0243,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:35] {2258} INFO - iteration 100, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:35] {2442} INFO -  at 96.9s,\testimator extra_tree's best error=0.0232,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:35] {2258} INFO - iteration 101, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:36] {2442} INFO -  at 97.4s,\testimator extra_tree's best error=0.0232,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:36] {2258} INFO - iteration 102, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:39:36] {2442} INFO -  at 97.8s,\testimator extra_tree's best error=0.0231,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:36] {2258} INFO - iteration 103, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:43] {2442} INFO -  at 104.9s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:43] {2258} INFO - iteration 104, current learner lrl1\n",
      "[flaml.automl.logger: 03-15 16:39:43] {2442} INFO -  at 105.1s,\testimator lrl1's best error=0.0385,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:43] {2258} INFO - iteration 105, current learner lrl1\n",
      "[flaml.automl.logger: 03-15 16:39:44] {2442} INFO -  at 105.2s,\testimator lrl1's best error=0.0385,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:44] {2258} INFO - iteration 106, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:39:44] {2442} INFO -  at 106.1s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:44] {2258} INFO - iteration 107, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:39:44] {2442} INFO -  at 106.2s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:44] {2258} INFO - iteration 108, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:39:45] {2442} INFO -  at 106.4s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:45] {2258} INFO - iteration 109, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:39:45] {2442} INFO -  at 106.6s,\testimator xgb_limitdepth's best error=0.0217,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:45] {2258} INFO - iteration 110, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:49] {2442} INFO -  at 111.0s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:49] {2258} INFO - iteration 111, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:39:50] {2442} INFO -  at 111.4s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:50] {2258} INFO - iteration 112, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:39:56] {2442} INFO -  at 117.4s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:56] {2258} INFO - iteration 113, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:39:56] {2442} INFO -  at 117.5s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:56] {2258} INFO - iteration 114, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:39:57] {2442} INFO -  at 118.3s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:39:57] {2258} INFO - iteration 115, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:40:00] {2442} INFO -  at 121.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:00] {2258} INFO - iteration 116, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:40:00] {2442} INFO -  at 121.9s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:00] {2258} INFO - iteration 117, current learner lrl1\n",
      "[flaml.automl.logger: 03-15 16:40:00] {2442} INFO -  at 122.0s,\testimator lrl1's best error=0.0385,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:00] {2258} INFO - iteration 118, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:40:01] {2442} INFO -  at 122.2s,\testimator xgboost's best error=0.0287,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:01] {2258} INFO - iteration 119, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:40:01] {2442} INFO -  at 123.0s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:01] {2258} INFO - iteration 120, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:40:05] {2442} INFO -  at 126.6s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:05] {2258} INFO - iteration 121, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:40:05] {2442} INFO -  at 126.7s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:05] {2258} INFO - iteration 122, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:40:06] {2442} INFO -  at 127.6s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:06] {2258} INFO - iteration 123, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:40:06] {2442} INFO -  at 127.7s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:06] {2258} INFO - iteration 124, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:40:06] {2442} INFO -  at 127.8s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:06] {2258} INFO - iteration 125, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:40:11] {2442} INFO -  at 132.4s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:11] {2258} INFO - iteration 126, current learner sgd\n",
      "[flaml.automl.logger: 03-15 16:40:11] {2442} INFO -  at 132.6s,\testimator sgd's best error=0.0492,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:11] {2258} INFO - iteration 127, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:40:12] {2442} INFO -  at 133.4s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:12] {2258} INFO - iteration 128, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:40:15] {2442} INFO -  at 136.3s,\testimator lgbm's best error=0.0207,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:15] {2258} INFO - iteration 129, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:40:21] {2442} INFO -  at 142.7s,\testimator lgbm's best error=0.0206,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:21] {2258} INFO - iteration 130, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:40:25] {2442} INFO -  at 146.8s,\testimator lgbm's best error=0.0206,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:25] {2258} INFO - iteration 131, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:40:27] {2442} INFO -  at 148.3s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:27] {2258} INFO - iteration 132, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:40:32] {2442} INFO -  at 153.5s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:32] {2258} INFO - iteration 133, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:40:32] {2442} INFO -  at 153.9s,\testimator extra_tree's best error=0.0231,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:32] {2258} INFO - iteration 134, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:40:33] {2442} INFO -  at 154.3s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:33] {2258} INFO - iteration 135, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:40:38] {2442} INFO -  at 159.5s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:38] {2258} INFO - iteration 136, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:40:46] {2442} INFO -  at 167.6s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:46] {2258} INFO - iteration 137, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:40:52] {2442} INFO -  at 173.8s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:52] {2258} INFO - iteration 138, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:40:53] {2442} INFO -  at 174.6s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:53] {2258} INFO - iteration 139, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:40:57] {2442} INFO -  at 179.0s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:57] {2258} INFO - iteration 140, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:40:58] {2442} INFO -  at 179.3s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:40:58] {2258} INFO - iteration 141, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:41:03] {2442} INFO -  at 184.8s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:03] {2258} INFO - iteration 142, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:05] {2442} INFO -  at 186.6s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:05] {2258} INFO - iteration 143, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:05] {2442} INFO -  at 187.0s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:05] {2258} INFO - iteration 144, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:07] {2442} INFO -  at 188.2s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:07] {2258} INFO - iteration 145, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:07] {2442} INFO -  at 188.6s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:07] {2258} INFO - iteration 146, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:08] {2442} INFO -  at 190.1s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:08] {2258} INFO - iteration 147, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:09] {2442} INFO -  at 190.4s,\testimator xgb_limitdepth's best error=0.0211,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:09] {2258} INFO - iteration 148, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:10] {2442} INFO -  at 192.1s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:10] {2258} INFO - iteration 149, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:41:22] {2442} INFO -  at 204.1s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:22] {2258} INFO - iteration 150, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:41:39] {2442} INFO -  at 220.9s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:39] {2258} INFO - iteration 151, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:41:40] {2442} INFO -  at 221.7s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:40] {2258} INFO - iteration 152, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:41:44] {2442} INFO -  at 225.5s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:44] {2258} INFO - iteration 153, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:41:45] {2442} INFO -  at 226.8s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:45] {2258} INFO - iteration 154, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:41:46] {2442} INFO -  at 227.7s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:46] {2258} INFO - iteration 155, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:41:50] {2442} INFO -  at 231.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:50] {2258} INFO - iteration 156, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:41:51] {2442} INFO -  at 233.2s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:51] {2258} INFO - iteration 157, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:41:56] {2442} INFO -  at 238.1s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:41:56] {2258} INFO - iteration 158, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:42:09] {2442} INFO -  at 250.3s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:09] {2258} INFO - iteration 159, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:42:14] {2442} INFO -  at 255.9s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:14] {2258} INFO - iteration 160, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:42:16] {2442} INFO -  at 257.6s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:16] {2258} INFO - iteration 161, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:42:17] {2442} INFO -  at 259.0s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:17] {2258} INFO - iteration 162, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:42:18] {2442} INFO -  at 259.2s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:18] {2258} INFO - iteration 163, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:42:21] {2442} INFO -  at 262.9s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:21] {2258} INFO - iteration 164, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:42:22] {2442} INFO -  at 263.6s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:22] {2258} INFO - iteration 165, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:42:23] {2442} INFO -  at 264.2s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:23] {2258} INFO - iteration 166, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:42:26] {2442} INFO -  at 268.0s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:26] {2258} INFO - iteration 167, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:42:31] {2442} INFO -  at 272.9s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:31] {2258} INFO - iteration 168, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:42:32] {2442} INFO -  at 273.3s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:32] {2258} INFO - iteration 169, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:42:32] {2442} INFO -  at 273.8s,\testimator extra_tree's best error=0.0231,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:32] {2258} INFO - iteration 170, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:42:36] {2442} INFO -  at 277.5s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:36] {2258} INFO - iteration 171, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:42:40] {2442} INFO -  at 281.4s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:40] {2258} INFO - iteration 172, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:42:41] {2442} INFO -  at 282.3s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:41] {2258} INFO - iteration 173, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:42:44] {2442} INFO -  at 285.8s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:42:44] {2258} INFO - iteration 174, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:43:03] {2442} INFO -  at 304.6s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:03] {2258} INFO - iteration 175, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:43:03] {2442} INFO -  at 305.0s,\testimator extra_tree's best error=0.0231,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:03] {2258} INFO - iteration 176, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:43:04] {2442} INFO -  at 305.5s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:04] {2258} INFO - iteration 177, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:43:09] {2442} INFO -  at 311.0s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:09] {2258} INFO - iteration 178, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:43:10] {2442} INFO -  at 311.2s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:10] {2258} INFO - iteration 179, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:43:15] {2442} INFO -  at 316.3s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:15] {2258} INFO - iteration 180, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:43:16] {2442} INFO -  at 317.4s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:16] {2258} INFO - iteration 181, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:43:16] {2442} INFO -  at 318.0s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:16] {2258} INFO - iteration 182, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:43:18] {2442} INFO -  at 320.1s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:18] {2258} INFO - iteration 183, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:43:22] {2442} INFO -  at 324.2s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:22] {2258} INFO - iteration 184, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:43:27] {2442} INFO -  at 328.6s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:27] {2258} INFO - iteration 185, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:43:28] {2442} INFO -  at 329.4s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:28] {2258} INFO - iteration 186, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:43:32] {2442} INFO -  at 334.0s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:32] {2258} INFO - iteration 187, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:43:32] {2442} INFO -  at 334.2s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:32] {2258} INFO - iteration 188, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:43:37] {2442} INFO -  at 338.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:37] {2258} INFO - iteration 189, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:43:39] {2442} INFO -  at 340.3s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:39] {2258} INFO - iteration 190, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:43:44] {2442} INFO -  at 345.5s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:44] {2258} INFO - iteration 191, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:43:57] {2442} INFO -  at 358.7s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:57] {2258} INFO - iteration 192, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:43:58] {2442} INFO -  at 359.8s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:43:58] {2258} INFO - iteration 193, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:44:04] {2442} INFO -  at 365.6s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:04] {2258} INFO - iteration 194, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:44:05] {2442} INFO -  at 366.4s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:05] {2258} INFO - iteration 195, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:44:09] {2442} INFO -  at 370.9s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:09] {2258} INFO - iteration 196, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:44:10] {2442} INFO -  at 371.4s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:10] {2258} INFO - iteration 197, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:44:14] {2442} INFO -  at 375.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:14] {2258} INFO - iteration 198, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:44:14] {2442} INFO -  at 376.0s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:14] {2258} INFO - iteration 199, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:44:51] {2442} INFO -  at 412.4s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:51] {2258} INFO - iteration 200, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:44:54] {2442} INFO -  at 416.1s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:54] {2258} INFO - iteration 201, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:44:59] {2442} INFO -  at 420.6s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:44:59] {2258} INFO - iteration 202, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:03] {2442} INFO -  at 424.7s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:03] {2258} INFO - iteration 203, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:07] {2442} INFO -  at 429.1s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:07] {2258} INFO - iteration 204, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:45:09] {2442} INFO -  at 430.9s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:09] {2258} INFO - iteration 205, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:45:10] {2442} INFO -  at 431.6s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:10] {2258} INFO - iteration 206, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:14] {2442} INFO -  at 435.3s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:14] {2258} INFO - iteration 207, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:18] {2442} INFO -  at 439.5s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:18] {2258} INFO - iteration 208, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:21] {2442} INFO -  at 442.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:21] {2258} INFO - iteration 209, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:45:27] {2442} INFO -  at 449.0s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:27] {2258} INFO - iteration 210, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:32] {2442} INFO -  at 454.0s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:32] {2258} INFO - iteration 211, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:37] {2442} INFO -  at 458.5s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:37] {2258} INFO - iteration 212, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:45:37] {2442} INFO -  at 458.9s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:37] {2258} INFO - iteration 213, current learner extra_tree\n",
      "[flaml.automl.logger: 03-15 16:45:38] {2442} INFO -  at 459.3s,\testimator extra_tree's best error=0.0231,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:38] {2258} INFO - iteration 214, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:42] {2442} INFO -  at 463.5s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:42] {2258} INFO - iteration 215, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:45:43] {2442} INFO -  at 464.7s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:43] {2258} INFO - iteration 216, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:48] {2442} INFO -  at 469.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:48] {2258} INFO - iteration 217, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:52] {2442} INFO -  at 473.6s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:52] {2258} INFO - iteration 218, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:45:52] {2442} INFO -  at 473.8s,\testimator xgboost's best error=0.0282,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:52] {2258} INFO - iteration 219, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:45:57] {2442} INFO -  at 478.8s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:57] {2258} INFO - iteration 220, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:45:58] {2442} INFO -  at 479.9s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:45:58] {2258} INFO - iteration 221, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:46:00] {2442} INFO -  at 481.6s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:46:00] {2258} INFO - iteration 222, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:46:00] {2442} INFO -  at 482.1s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:46:00] {2258} INFO - iteration 223, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 03-15 16:46:03] {2442} INFO -  at 484.2s,\testimator xgb_limitdepth's best error=0.0210,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:46:03] {2258} INFO - iteration 224, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:46:18] {2442} INFO -  at 499.8s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:46:18] {2258} INFO - iteration 225, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:46:55] {2442} INFO -  at 537.1s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:46:55] {2258} INFO - iteration 226, current learner catboost\n",
      "[flaml.automl.logger: 03-15 16:47:47] {2442} INFO -  at 588.2s,\testimator catboost's best error=0.0203,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:47] {2258} INFO - iteration 227, current learner lgbm\n",
      "[flaml.automl.logger: 03-15 16:47:50] {2442} INFO -  at 591.5s,\testimator lgbm's best error=0.0205,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:50] {2258} INFO - iteration 228, current learner rf\n",
      "[flaml.automl.logger: 03-15 16:47:50] {2442} INFO -  at 592.1s,\testimator rf's best error=0.0220,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:50] {2258} INFO - iteration 229, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:47:51] {2442} INFO -  at 593.2s,\testimator xgboost's best error=0.0251,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:51] {2258} INFO - iteration 230, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:47:52] {2442} INFO -  at 593.9s,\testimator xgboost's best error=0.0251,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:52] {2258} INFO - iteration 231, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:47:53] {2442} INFO -  at 595.2s,\testimator xgboost's best error=0.0251,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:53] {2258} INFO - iteration 232, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:47:55] {2442} INFO -  at 596.8s,\testimator xgboost's best error=0.0251,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:55] {2258} INFO - iteration 233, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:47:56] {2442} INFO -  at 597.4s,\testimator xgboost's best error=0.0232,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:56] {2258} INFO - iteration 234, current learner xgboost\n",
      "[flaml.automl.logger: 03-15 16:47:58] {2442} INFO -  at 600.0s,\testimator xgboost's best error=0.0217,\tbest estimator catboost's best error=0.0203\n",
      "[flaml.automl.logger: 03-15 16:47:59] {2685} INFO - retrain catboost for 0.6s\n",
      "[flaml.automl.logger: 03-15 16:47:59] {2688} INFO - retrained model: <catboost.core.CatBoostClassifier object at 0x000001E911946290>\n",
      "[flaml.automl.logger: 03-15 16:47:59] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 03-15 16:47:59] {1986} INFO - Time taken to find the best model: 375.76807975769043\n",
      "أفضل نموذج: catboost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      1693\n",
      "           1       0.90      0.93      0.91      1697\n",
      "\n",
      "    accuracy                           0.91      3390\n",
      "   macro avg       0.91      0.91      0.91      3390\n",
      "weighted avg       0.91      0.91      0.91      3390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from flaml import AutoML\n",
    "\n",
    "#automl = AutoML()\n",
    "#automl.fit(X_train, y_train, task=\"classification\", time_budget=600)  # التدريب لمدة 10 دقائق\n",
    "#print(\"أفضل نموذج:\", automl.best_estimator)\n",
    "\n",
    "#from sklearn.metrics import classification_report\n",
    "#y_pred = automl.predict(X_test)\n",
    "#print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1d2a05a9fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.1, verbose=False)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دقة النموذج: 0.8988200589970502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      1697\n",
      "           1       0.89      0.91      0.90      1693\n",
      "\n",
      "    accuracy                           0.90      3390\n",
      "   macro avg       0.90      0.90      0.90      3390\n",
      "weighted avg       0.90      0.90      0.90      3390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_p=model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_p)\n",
    "print(\"دقة النموذج:\", accuracy)\n",
    "\n",
    "print(classification_report(y_test, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# إدخال بيانات شخص معين للاختبار\n",
    "real_values = [[6, 120, 20, 45, 0, 0]]  # HbA1c_level, blood_glucose_level, bmi, age, hypertension, heart_disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التوقع: Non-Diabetic (0.01%)\n"
     ]
    }
   ],
   "source": [
    "# توقع الاحتمالات للفئتين (الإصابة بالسكري أو عدمها)\n",
    "probabilities = model.predict_proba(real_values)\n",
    "\n",
    "# استخراج نسبة الإصابة بالسكري (العمود الثاني يمثل احتمالية الفئة 1 \"Diabetic\")\n",
    "diabetes_probability = probabilities[0][1]\n",
    "\n",
    "# طباعة النتيجة مع النسبة المئوية\n",
    "print(f\"التوقع: {'Diabetic' if diabetes_probability >= 0.5 else 'Non-Diabetic'} ({diabetes_probability * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Sample Data (6 features)\n",
    "X_train = np.random.rand(1000, 6)  \n",
    "y_train = np.random.randint(0, 2, size=(1000,))  \n",
    "\n",
    "# Train CatBoost Model\n",
    "catboost_model = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, verbose=0)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Get CatBoost Predictions (soft probabilities)\n",
    "y_train_pred = catboost_model.predict_proba(X_train)  # Shape (1000, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as catboost_keras_modell.h5\n"
     ]
    }
   ],
   "source": [
    "keras_model.save(\"C:/Users/MINA/Downloads/catboost_keras_model.h5\")\n",
    "print(\"Model saved as catboost_keras_modell.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(\"C:/Users/MINA/Downloads/catboost_keras_model.h5\")\n",
    "\n",
    "# Test with Real Input\n",
    "real_values = np.array([[6, 120, 20, 45, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "التوقع: Non-Diabetic (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Get probabilities from the model\n",
    "probabilities = loaded_model.predict(real_values)\n",
    "\n",
    "# Extract probability for the \"Diabetic\" class (index 1)\n",
    "diabetes_probability = probabilities[0][1]  \n",
    "\n",
    "# Print result in Arabic\n",
    "print(f\"التوقع: {'Diabetic' if diabetes_probability >= 0.5 else 'Non-Diabetic'} ({diabetes_probability * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HbA1c_level  blood_glucose_level   bmi  age  hypertension  heart_disease\n",
      "0          6.6                  116  30.3   64             1              1\n",
      "1          9.7                   81  39.3   75             1              0\n",
      "2          8.5                  131  27.8   44             0              0\n",
      "3          7.8                  149  25.2   70             0              0\n",
      "4          5.4                  152  29.4   59             0              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# تحديد عدد السجلات\n",
    "num_samples = 1000\n",
    "\n",
    "# توليد بيانات عشوائية ضمن نطاق القيم الحقيقية\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    \"HbA1c_level\": np.round(np.random.uniform(4.5, 10.0, num_samples), 1),  # القيم بين 4.5 و 10\n",
    "    \"blood_glucose_level\": np.random.randint(70, 250, num_samples),  # القيم بين 70 و 250\n",
    "    \"bmi\": np.round(np.random.uniform(18.5, 40.0, num_samples), 1),  # القيم بين 18.5 و 40\n",
    "    \"age\": np.random.randint(18, 90, num_samples),  # العمر بين 18 و 90 سنة\n",
    "    \"hypertension\": np.random.choice([0, 1], num_samples, p=[0.85, 0.15]),  # 15% لديهم ارتفاع ضغط الدم\n",
    "    \"heart_disease\": np.random.choice([0, 1], num_samples, p=[0.90, 0.10]),  # 10% لديهم أمراض القلب\n",
    "}\n",
    "\n",
    "# إنشاء DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# عرض أول 5 سجلات\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HbA1c_level  blood_glucose_level   bmi   age  hypertension  heart_disease\n",
      "0          6.7                103.0  23.6  48.0             0              0\n",
      "1          5.8                102.0  25.5  50.0             1              1\n",
      "2          7.0                 70.0  24.0  51.0             1              0\n",
      "3          8.3                107.0  27.6  64.0             1              1\n",
      "4          5.6                149.0  33.0  39.0             1              0\n",
      "\n",
      "Hypertension Distribution:\n",
      " 0    0.512\n",
      "1    0.488\n",
      "Name: hypertension, dtype: float64\n",
      "\n",
      "Heart Disease Distribution:\n",
      " 1    0.5034\n",
      "0    0.4966\n",
      "Name: heart_disease, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# تحديد عدد السجلات الأكبر لزيادة التوازن\n",
    "num_samples = 5000  \n",
    "\n",
    "# توليد بيانات عشوائية ضمن نطاق القيم الحقيقية\n",
    "np.random.seed(42)\n",
    "\n",
    "# توليد قيم HbA1c بتوزيع طبيعي حول القيم الشائعة\n",
    "HbA1c_mean, HbA1c_std = 6.0, 1.5  # متوسط HbA1c حوالي 6 مع انحراف معياري 1.5\n",
    "HbA1c_values = np.clip(np.random.normal(HbA1c_mean, HbA1c_std, num_samples), 4.5, 10.0)\n",
    "\n",
    "# توليد قيم السكر في الدم بتوزيع طبيعي\n",
    "glucose_mean, glucose_std = 120, 40  # متوسط السكر في الدم 120 مع انحراف معياري 40\n",
    "glucose_values = np.clip(np.random.normal(glucose_mean, glucose_std, num_samples), 70, 250)\n",
    "\n",
    "# توليد قيم مؤشر كتلة الجسم (BMI) بتوزيع طبيعي\n",
    "bmi_mean, bmi_std = 27, 5  # متوسط BMI حوالي 27 مع انحراف 5\n",
    "bmi_values = np.clip(np.random.normal(bmi_mean, bmi_std, num_samples), 18.5, 40.0)\n",
    "\n",
    "# توليد قيم العمر بتوزيع طبيعي مع قص القيم غير الواقعية\n",
    "age_mean, age_std = 50, 15  # متوسط العمر 50 مع انحراف 15\n",
    "age_values = np.clip(np.random.normal(age_mean, age_std, num_samples), 18, 90)\n",
    "\n",
    "# تحقيق التوازن في hypertension و heart_disease بحيث تكون النسب 50%-50%\n",
    "hypertension_values = np.random.choice([0, 1], num_samples, p=[0.5, 0.5])\n",
    "heart_disease_values = np.random.choice([0, 1], num_samples, p=[0.5, 0.5])\n",
    "\n",
    "# إنشاء DataFrame\n",
    "data = {\n",
    "    \"HbA1c_level\": np.round(HbA1c_values, 1),\n",
    "    \"blood_glucose_level\": np.round(glucose_values),\n",
    "    \"bmi\": np.round(bmi_values, 1),\n",
    "    \"age\": np.round(age_values),\n",
    "    \"hypertension\": hypertension_values,\n",
    "    \"heart_disease\": heart_disease_values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# عرض أول 5 سجلات\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nHypertension Distribution:\\n\", df[\"hypertension\"].value_counts(normalize=True))\n",
    "print(\"\\nHeart Disease Distribution:\\n\", df[\"heart_disease\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HbA1c_level  blood_glucose_level   bmi   age  hypertension  heart_disease  \\\n",
      "0          6.7                103.0  23.6  48.0             0              0   \n",
      "1          5.8                102.0  25.5  50.0             1              1   \n",
      "2          7.0                 70.0  24.0  51.0             1              0   \n",
      "3          8.3                107.0  27.6  64.0             1              1   \n",
      "4          5.6                149.0  33.0  39.0             1              0   \n",
      "\n",
      "   diabetes  \n",
      "0         1  \n",
      "1         0  \n",
      "2         1  \n",
      "3         1  \n",
      "4         1  \n",
      "\n",
      "Diabetes Distribution:\n",
      " 1    0.6464\n",
      "0    0.3536\n",
      "Name: diabetes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 5000  \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "HbA1c_mean, HbA1c_std = 6.0, 1.5  \n",
    "HbA1c_values = np.clip(np.random.normal(HbA1c_mean, HbA1c_std, num_samples), 4.5, 10.0)\n",
    "\n",
    "glucose_mean, glucose_std = 120, 40  \n",
    "glucose_values = np.clip(np.random.normal(glucose_mean, glucose_std, num_samples), 70, 250)\n",
    "\n",
    "bmi_mean, bmi_std = 27, 5  \n",
    "bmi_values = np.clip(np.random.normal(bmi_mean, bmi_std, num_samples), 18.5, 40.0)\n",
    "\n",
    "age_mean, age_std = 50, 15  \n",
    "age_values = np.clip(np.random.normal(age_mean, age_std, num_samples), 18, 90)\n",
    "\n",
    "hypertension_values = np.random.choice([0, 1], num_samples, p=[0.5, 0.5])\n",
    "heart_disease_values = np.random.choice([0, 1], num_samples, p=[0.5, 0.5])\n",
    "\n",
    "diabetes_values = np.where(\n",
    "    (HbA1c_values >= 6.5) | (glucose_values >= 126), 1, 0\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"HbA1c_level\": np.round(HbA1c_values, 1),\n",
    "    \"blood_glucose_level\": np.round(glucose_values),\n",
    "    \"bmi\": np.round(bmi_values, 1),\n",
    "    \"age\": np.round(age_values),\n",
    "    \"hypertension\": hypertension_values,\n",
    "    \"heart_disease\": heart_disease_values,\n",
    "    \"diabetes\": diabetes_values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDiabetes Distribution:\\n\", df[\"diabetes\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HbA1c_level  blood_glucose_level   bmi   age  hypertension  heart_disease  \\\n",
      "0          6.4                100.0  23.6  48.0             0              0   \n",
      "1          5.6                 99.0  25.5  50.0             1              1   \n",
      "2          6.6                 70.0  24.0  51.0             1              0   \n",
      "3          7.8                103.0  27.6  64.0             1              1   \n",
      "4          5.5                141.0  33.0  39.0             1              0   \n",
      "\n",
      "   diabetes  \n",
      "0         0  \n",
      "1         0  \n",
      "2         1  \n",
      "3         1  \n",
      "4         1  \n",
      "\n",
      "Diabetes Distribution:\n",
      " 1    0.56\n",
      "0    0.44\n",
      "Name: diabetes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 5000  \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "HbA1c_mean, HbA1c_std = 5.8, 1.3  \n",
    "HbA1c_values = np.clip(np.random.normal(HbA1c_mean, HbA1c_std, num_samples), 4.5, 10.0)\n",
    "\n",
    "glucose_mean, glucose_std = 115, 35  \n",
    "glucose_values = np.clip(np.random.normal(glucose_mean, glucose_std, num_samples), 70, 250)\n",
    "\n",
    "bmi_mean, bmi_std = 27, 5  \n",
    "bmi_values = np.clip(np.random.normal(bmi_mean, bmi_std, num_samples), 18.5, 40.0)\n",
    "\n",
    "age_mean, age_std = 50, 15  \n",
    "age_values = np.clip(np.random.normal(age_mean, age_std, num_samples), 18, 90)\n",
    "\n",
    "hypertension_values = np.random.choice([0, 1], num_samples, p=[0.5, 0.5])\n",
    "heart_disease_values = np.random.choice([0, 1], num_samples, p=[0.5, 0.5])\n",
    "\n",
    "diabetes_values = np.where(\n",
    "    (HbA1c_values >= 6.5) | (glucose_values >= 126), 1, 0\n",
    ")\n",
    "\n",
    "unique, counts = np.unique(diabetes_values, return_counts=True)\n",
    "diabetes_ratio = counts[1] / num_samples\n",
    "\n",
    "if diabetes_ratio > 0.51:\n",
    "    drop_indices = np.random.choice(df[df[\"diabetes\"] == 1].index, size=int(counts[1] - 0.51 * num_samples), replace=False)\n",
    "    df = df.drop(drop_indices)\n",
    "\n",
    "elif diabetes_ratio < 0.49:\n",
    "    drop_indices = np.random.choice(df[df[\"diabetes\"] == 0].index, size=int(counts[0] - 0.49 * num_samples), replace=False)\n",
    "    df = df.drop(drop_indices)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "data = {\n",
    "    \"HbA1c_level\": np.round(HbA1c_values, 1),\n",
    "    \"blood_glucose_level\": np.round(glucose_values),\n",
    "    \"bmi\": np.round(bmi_values, 1),\n",
    "    \"age\": np.round(age_values),\n",
    "    \"hypertension\": hypertension_values,\n",
    "    \"heart_disease\": heart_disease_values,\n",
    "    \"diabetes\": diabetes_values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDiabetes Distribution:\\n\", df[\"diabetes\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5991 - loss: 0.6550 - val_accuracy: 0.8850 - val_loss: 0.4495\n",
      "Epoch 2/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7806 - loss: 0.4816 - val_accuracy: 0.9110 - val_loss: 0.2918\n",
      "Epoch 3/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8440 - loss: 0.3638 - val_accuracy: 0.9170 - val_loss: 0.2211\n",
      "Epoch 4/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8571 - loss: 0.3227 - val_accuracy: 0.9240 - val_loss: 0.1869\n",
      "Epoch 5/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8848 - loss: 0.2733 - val_accuracy: 0.9340 - val_loss: 0.1596\n",
      "Epoch 6/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8784 - loss: 0.2818 - val_accuracy: 0.9520 - val_loss: 0.1405\n",
      "Epoch 7/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9070 - loss: 0.2228 - val_accuracy: 0.9610 - val_loss: 0.1204\n",
      "Epoch 8/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9142 - loss: 0.2093 - val_accuracy: 0.9680 - val_loss: 0.1044\n",
      "Epoch 9/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9131 - loss: 0.1991 - val_accuracy: 0.9690 - val_loss: 0.0970\n",
      "Epoch 10/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9324 - loss: 0.1709 - val_accuracy: 0.9710 - val_loss: 0.0868\n",
      "Epoch 11/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9301 - loss: 0.1800 - val_accuracy: 0.9780 - val_loss: 0.0787\n",
      "Epoch 12/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9329 - loss: 0.1654 - val_accuracy: 0.9780 - val_loss: 0.0723\n",
      "Epoch 13/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1450 - val_accuracy: 0.9820 - val_loss: 0.0682\n",
      "Epoch 14/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9488 - loss: 0.1303 - val_accuracy: 0.9810 - val_loss: 0.0605\n",
      "Epoch 15/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1211 - val_accuracy: 0.9820 - val_loss: 0.0575\n",
      "Epoch 16/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9549 - loss: 0.1144 - val_accuracy: 0.9850 - val_loss: 0.0541\n",
      "Epoch 17/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9529 - loss: 0.1206 - val_accuracy: 0.9840 - val_loss: 0.0522\n",
      "Epoch 18/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - accuracy: 0.9620 - loss: 0.1022 - val_accuracy: 0.9870 - val_loss: 0.0490\n",
      "Epoch 19/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9556 - loss: 0.1089 - val_accuracy: 0.9900 - val_loss: 0.0477\n",
      "Epoch 20/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9502 - loss: 0.1283 - val_accuracy: 0.9880 - val_loss: 0.0480\n",
      "Epoch 21/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9553 - loss: 0.1145 - val_accuracy: 0.9880 - val_loss: 0.0455\n",
      "Epoch 22/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9579 - loss: 0.1017 - val_accuracy: 0.9890 - val_loss: 0.0434\n",
      "Epoch 23/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9574 - loss: 0.1060 - val_accuracy: 0.9900 - val_loss: 0.0425\n",
      "Epoch 24/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9640 - loss: 0.0987 - val_accuracy: 0.9890 - val_loss: 0.0408\n",
      "Epoch 25/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9725 - loss: 0.0836 - val_accuracy: 0.9840 - val_loss: 0.0397\n",
      "Epoch 26/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.0957 - val_accuracy: 0.9850 - val_loss: 0.0396\n",
      "Epoch 27/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9674 - loss: 0.0844 - val_accuracy: 0.9870 - val_loss: 0.0390\n",
      "Epoch 28/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9622 - loss: 0.0941 - val_accuracy: 0.9870 - val_loss: 0.0382\n",
      "Epoch 29/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9693 - loss: 0.0839 - val_accuracy: 0.9870 - val_loss: 0.0363\n",
      "Epoch 30/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9683 - loss: 0.0769 - val_accuracy: 0.9870 - val_loss: 0.0353\n",
      "Epoch 31/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9651 - loss: 0.0839 - val_accuracy: 0.9870 - val_loss: 0.0357\n",
      "Epoch 32/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9736 - loss: 0.0742 - val_accuracy: 0.9890 - val_loss: 0.0336\n",
      "Epoch 33/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9756 - loss: 0.0655 - val_accuracy: 0.9870 - val_loss: 0.0322\n",
      "Epoch 34/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9778 - loss: 0.0669 - val_accuracy: 0.9880 - val_loss: 0.0320\n",
      "Epoch 35/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9761 - loss: 0.0706 - val_accuracy: 0.9880 - val_loss: 0.0309\n",
      "Epoch 36/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9716 - loss: 0.0733 - val_accuracy: 0.9900 - val_loss: 0.0297\n",
      "Epoch 37/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9717 - loss: 0.0724 - val_accuracy: 0.9880 - val_loss: 0.0309\n",
      "Epoch 38/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9783 - loss: 0.0594 - val_accuracy: 0.9910 - val_loss: 0.0288\n",
      "Epoch 39/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9687 - loss: 0.0664 - val_accuracy: 0.9910 - val_loss: 0.0290\n",
      "Epoch 40/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9773 - loss: 0.0601 - val_accuracy: 0.9870 - val_loss: 0.0313\n",
      "Epoch 41/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9754 - loss: 0.0607 - val_accuracy: 0.9880 - val_loss: 0.0281\n",
      "Epoch 42/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9766 - loss: 0.0557 - val_accuracy: 0.9880 - val_loss: 0.0283\n",
      "Epoch 43/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9721 - loss: 0.0700 - val_accuracy: 0.9870 - val_loss: 0.0282\n",
      "Epoch 44/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9748 - loss: 0.0608 - val_accuracy: 0.9850 - val_loss: 0.0305\n",
      "Epoch 45/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9741 - loss: 0.0682 - val_accuracy: 0.9890 - val_loss: 0.0275\n",
      "Epoch 46/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9804 - loss: 0.0495 - val_accuracy: 0.9860 - val_loss: 0.0278\n",
      "Epoch 47/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9755 - loss: 0.0603 - val_accuracy: 0.9890 - val_loss: 0.0277\n",
      "Epoch 48/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9733 - loss: 0.0617 - val_accuracy: 0.9870 - val_loss: 0.0280\n",
      "Epoch 49/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0598 - val_accuracy: 0.9890 - val_loss: 0.0266\n",
      "Epoch 50/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9770 - loss: 0.0617 - val_accuracy: 0.9890 - val_loss: 0.0267\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Accuracy: 0.989\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       440\n",
      "           1       0.99      0.99      0.99       560\n",
      "\n",
      "    accuracy                           0.99      1000\n",
      "   macro avg       0.99      0.99      0.99      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[432   8]\n",
      " [  3 557]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "num_samples = 5000\n",
    "np.random.seed(42)\n",
    "\n",
    "HbA1c_values = np.clip(np.random.normal(5.8, 1.3, num_samples), 4.5, 10.0)\n",
    "glucose_values = np.clip(np.random.normal(115, 35, num_samples), 70, 250)\n",
    "bmi_values = np.clip(np.random.normal(27, 5, num_samples), 18.5, 40.0)\n",
    "age_values = np.clip(np.random.normal(50, 15, num_samples), 18, 90)\n",
    "\n",
    "diabetes_values = np.where((HbA1c_values >= 6.5) | (glucose_values >= 126), 1, 0)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"HbA1c_level\": np.round(HbA1c_values, 1),\n",
    "    \"blood_glucose_level\": np.round(glucose_values),\n",
    "    \"bmi\": np.round(bmi_values, 1),\n",
    "    \"age\": np.round(age_values),\n",
    "    \"diabetes\": diabetes_values\n",
    "})\n",
    "\n",
    "X = df.drop(columns=[\"diabetes\"])\n",
    "y = df[\"diabetes\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dropout(0.3),  \n",
    "    keras.layers.Dense(8, activation=\"relu\"), \n",
    "    keras.layers.Dropout(0.3), \n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=1)\n",
    "\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Sample 1: HbA1c=7.2, Glucose=150.0, BMI=30.0, Age=55.0 -> Diabetes Prediction: Positive\n",
      "Sample 2: HbA1c=5.5, Glucose=90.0, BMI=25.0, Age=40.0 -> Diabetes Prediction: Negative\n",
      "Sample 3: HbA1c=8.0, Glucose=180.0, BMI=35.0, Age=65.0 -> Diabetes Prediction: Positive\n",
      "Sample 4: HbA1c=6.3, Glucose=110.0, BMI=28.0, Age=50.0 -> Diabetes Prediction: Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "real_life_samples = np.array([\n",
    "    [7.2, 150, 30, 55],  \n",
    "    [5.5, 90, 25, 40],   \n",
    "    [8.0, 180, 35, 65],    \n",
    "    [6.3, 110, 28, 50],   \n",
    "])\n",
    "\n",
    "real_life_samples_scaled = scaler.transform(real_life_samples)\n",
    "\n",
    "predictions = model.predict(real_life_samples_scaled)\n",
    "\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "for i, (sample, pred) in enumerate(zip(real_life_samples, predicted_classes)):\n",
    "    print(f\"Sample {i+1}: HbA1c={sample[0]}, Glucose={sample[1]}, BMI={sample[2]}, Age={sample[3]} -> Diabetes Prediction: {'Positive' if pred[0] == 1 else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Sample 1: HbA1c=7.5, Glucose=160.0, BMI=32.0, Age=60.0 -> Diabetes Prediction: Positive\n",
      "Sample 2: HbA1c=5.0, Glucose=85.0, BMI=22.0, Age=35.0 -> Diabetes Prediction: Negative\n",
      "Sample 3: HbA1c=9.0, Glucose=200.0, BMI=38.0, Age=70.0 -> Diabetes Prediction: Positive\n",
      "Sample 4: HbA1c=6.1, Glucose=105.0, BMI=26.0, Age=45.0 -> Diabetes Prediction: Negative\n",
      "Sample 5: HbA1c=10.0, Glucose=240.0, BMI=40.0, Age=75.0 -> Diabetes Prediction: Positive\n",
      "Sample 6: HbA1c=4.8, Glucose=78.0, BMI=19.0, Age=30.0 -> Diabetes Prediction: Negative\n",
      "Sample 7: HbA1c=7.0, Glucose=140.0, BMI=29.0, Age=55.0 -> Diabetes Prediction: Positive\n",
      "Sample 8: HbA1c=8.5, Glucose=190.0, BMI=36.0, Age=65.0 -> Diabetes Prediction: Positive\n",
      "Sample 9: HbA1c=5.7, Glucose=95.0, BMI=23.0, Age=42.0 -> Diabetes Prediction: Negative\n",
      "Sample 10: HbA1c=6.8, Glucose=130.0, BMI=27.0, Age=50.0 -> Diabetes Prediction: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# بيانات اختبار موسعة تحتوي على حالات متنوعة للسكري\n",
    "real_life_samples = np.array([\n",
    "    [7.5, 160, 32, 60],  # حالة متوسطة لمرض السكري\n",
    "    [5.0, 85, 22, 35],   # شخص سليم\n",
    "    [9.0, 200, 38, 70],  # حالة سكري حادة\n",
    "    [6.1, 105, 26, 45],  # شخص في الحدود الطبيعية\n",
    "    [10.0, 240, 40, 75], # مريض سكري بمؤشرات مرتفعة جدًا\n",
    "    [4.8, 78, 19, 30],   # شخص سليم وصغير في العمر\n",
    "    [7.0, 140, 29, 55],  # حالة قريبة من السكري\n",
    "    [8.5, 190, 36, 65],  # مريض سكري مرتفع المخاطر\n",
    "    [5.7, 95, 23, 42],   # شخص في الحد الطبيعي\n",
    "    [6.8, 130, 27, 50],  # حالة غير مؤكدة (بين الطبيعي والسكري)\n",
    "])\n",
    "\n",
    "# تطبيع البيانات باستخدام StandardScaler\n",
    "real_life_samples_scaled = scaler.transform(real_life_samples)\n",
    "\n",
    "# توقع النتائج باستخدام النموذج المدرب\n",
    "predictions = model.predict(real_life_samples_scaled)\n",
    "\n",
    "# تحويل القيم لاحتمالات\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# عرض النتائج\n",
    "for i, (sample, pred) in enumerate(zip(real_life_samples, predicted_classes)):\n",
    "    print(f\"Sample {i+1}: HbA1c={sample[0]}, Glucose={sample[1]}, BMI={sample[2]}, Age={sample[3]} -> Diabetes Prediction: {'Positive' if pred[0] == 1 else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Sample 1: HbA1c=7.5, Glucose=160.0, BMI=32.0, Age=60.0 -> Diabetes Prediction: Positive (100.00%)\n",
      "Sample 2: HbA1c=5.0, Glucose=85.0, BMI=22.0, Age=35.0 -> Diabetes Prediction: Negative (0.03%)\n",
      "Sample 3: HbA1c=9.0, Glucose=200.0, BMI=38.0, Age=70.0 -> Diabetes Prediction: Positive (100.00%)\n",
      "Sample 4: HbA1c=6.1, Glucose=105.0, BMI=26.0, Age=45.0 -> Diabetes Prediction: Negative (4.05%)\n",
      "Sample 5: HbA1c=10.0, Glucose=240.0, BMI=40.0, Age=75.0 -> Diabetes Prediction: Positive (100.00%)\n",
      "Sample 6: HbA1c=4.8, Glucose=78.0, BMI=19.0, Age=30.0 -> Diabetes Prediction: Negative (0.01%)\n",
      "Sample 7: HbA1c=7.0, Glucose=140.0, BMI=29.0, Age=55.0 -> Diabetes Prediction: Positive (100.00%)\n",
      "Sample 8: HbA1c=8.5, Glucose=190.0, BMI=36.0, Age=65.0 -> Diabetes Prediction: Positive (100.00%)\n",
      "Sample 9: HbA1c=5.7, Glucose=95.0, BMI=23.0, Age=42.0 -> Diabetes Prediction: Negative (0.64%)\n",
      "Sample 10: HbA1c=6.8, Glucose=130.0, BMI=27.0, Age=50.0 -> Diabetes Prediction: Positive (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MINA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# بيانات اختبار موسعة تحتوي على حالات متنوعة للسكري\n",
    "real_life_samples = np.array([\n",
    "    [7.5, 160, 32, 60],  # حالة متوسطة لمرض السكري\n",
    "    [5.0, 85, 22, 35],   # شخص سليم\n",
    "    [9.0, 200, 38, 70],  # حالة سكري حادة\n",
    "    [6.1, 105, 26, 45],  # شخص في الحدود الطبيعية\n",
    "    [10.0, 240, 40, 75], # مريض سكري بمؤشرات مرتفعة جدًا\n",
    "    [4.8, 78, 19, 30],   # شخص سليم وصغير في العمر\n",
    "    [7.0, 140, 29, 55],  # حالة قريبة من السكري\n",
    "    [8.5, 190, 36, 65],  # مريض سكري مرتفع المخاطر\n",
    "    [5.7, 95, 23, 42],   # شخص في الحد الطبيعي\n",
    "    [6.8, 130, 27, 50],  # حالة غير مؤكدة (بين الطبيعي والسكري)\n",
    "])\n",
    "\n",
    "# تطبيع البيانات باستخدام StandardScaler\n",
    "real_life_samples_scaled = scaler.transform(real_life_samples)\n",
    "\n",
    "# توقع النتائج باستخدام النموذج المدرب\n",
    "predictions = model.predict(real_life_samples_scaled)\n",
    "\n",
    "# تحويل القيم لاحتمالات ونسبة مئوية\n",
    "predicted_probabilities = predictions * 100  # تحويل القيم إلى نسبة مئوية\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")  # تحويل إلى 0 أو 1\n",
    "\n",
    "# عرض النتائج مع النسبة المئوية\n",
    "for i, (sample, prob, pred) in enumerate(zip(real_life_samples, predicted_probabilities, predicted_classes)):\n",
    "    print(f\"Sample {i+1}: HbA1c={sample[0]}, Glucose={sample[1]}, BMI={sample[2]}, Age={sample[3]} -> \"\n",
    "          f\"Diabetes Prediction: {'Positive' if pred[0] == 1 else 'Negative'} ({prob[0]:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"C:/Users/MINA/Downloads/Createdmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
